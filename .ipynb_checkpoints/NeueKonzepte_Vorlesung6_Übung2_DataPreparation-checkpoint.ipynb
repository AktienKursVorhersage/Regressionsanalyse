{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "      0   1   2   3    4\n",
      "0    96  12  82  71   64\n",
      "1    88  76  10  78   41\n",
      "2     5  64  41  61   93\n",
      "3    65   6  13  94   41\n",
      "4    50  84   9  30   60\n",
      "5    35  45  73  20   11\n",
      "6    77  96  88   1   74\n",
      "7     9  63  37  84  100\n",
      "8    29  64   8  11   53\n",
      "9    57  39  74  53   19\n",
      "10   72  16  45   1   13\n",
      "11   18  76  80  98   94\n",
      "12   25  37  64  20   36\n",
      "13   31  11  61  21   28\n",
      "14    9  87  27  88   47\n",
      "15   48  55  87  10   46\n",
      "16    3  19  59  93   12\n",
      "17   11  95  36  29    4\n",
      "18   84  85  48  15   70\n",
      "19   61  70  52   7   89\n",
      "20   72  69  24  36   80\n",
      "21   99  68  83  58   78\n",
      "22   47   4  47  30   87\n",
      "23   22  22  82  24   95\n",
      "24   72  21  28  76    6\n",
      "25   50  87  90  64   83\n",
      "26   78   4  57  15   50\n",
      "27   88  53  14  48   50\n",
      "28   25  21  65  53   61\n",
      "29   48  30  61  54   12\n",
      "..  ...  ..  ..  ..  ...\n",
      "70   53   8  41  74   87\n",
      "71   15  50  98  26   58\n",
      "72   41  18  33  84   98\n",
      "73   28  48  14  71   16\n",
      "74   93  19  95  49   66\n",
      "75   83  35   6  47   84\n",
      "76   28  27  21  88   85\n",
      "77   18  60  65  45    5\n",
      "78   52  50  75  83   38\n",
      "79   54  94  74   6   38\n",
      "80   57  36  16  41   43\n",
      "81   72  38  47  72   92\n",
      "82   98  37  44  28   67\n",
      "83   58   4  56  71   42\n",
      "84   68  73  89  68   76\n",
      "85   70  93  21  16   58\n",
      "86   10  70  98  92   52\n",
      "87   55  46  39  16   43\n",
      "88   62   9   4  89   73\n",
      "89   42  25  94  29   96\n",
      "90   44  49  70  43   67\n",
      "91   83  67  89  79   15\n",
      "92   54  47  15  28   69\n",
      "93   22  39  43  31   89\n",
      "94   80  57  66  94   38\n",
      "95   88  67  17  61   26\n",
      "96  100  31  42  73   46\n",
      "97   27  88  66  61   90\n",
      "98   71  34  60  29   17\n",
      "99   50  96  42  12   87\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(101)\n",
    "mat = np.random.randint(1,101,(100,5))\n",
    "df = pd.DataFrame(mat)\n",
    "print('Before:')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescale Data\n",
    "\n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale.\n",
    "\n",
    "Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms in used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like K-Nearest Neighbors.\n",
    "\n",
    "#### After rescaling you can see that all of the values are in the range between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Rescaling:\n",
      "[[ 0.959  0.104  0.821  0.722  0.633]\n",
      " [ 0.876  0.771  0.063  0.794  0.398]\n",
      " [ 0.021  0.646  0.389  0.619  0.929]\n",
      " [ 0.639  0.042  0.095  0.959  0.398]\n",
      " [ 0.485  0.854  0.053  0.299  0.592]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize data (0 mean, 1 stdev)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "rescaled_data = scaler.fit_transform(df)\n",
    "\n",
    "print('After Rescaling:')\n",
    "np.set_printoptions(precision=3)\n",
    "print(rescaled_data[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize Data\n",
    "\n",
    "Standardization is a useful technique to transform attributes with a Gaussian distribution and differing means and standard deviations to a standard Gaussian distribution with a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "It is most suitable for techniques that assume a Gaussian distribution in the input variables and work better with rescaled data, such as linear regression, logistic regression and linear discriminate analysis.\n",
    "\n",
    "#### The values for each attribute now have a mean value of 0 and a standard deviation of 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Standardization:\n",
      "[[ 1.554 -1.403  1.041  0.667  0.303]\n",
      " [ 1.264  0.911 -1.554  0.91  -0.498]\n",
      " [-1.744  0.477 -0.437  0.32   1.314]\n",
      " [ 0.431 -1.62  -1.446  1.464 -0.498]\n",
      " [-0.113  1.2   -1.591 -0.754  0.164]]\n"
     ]
    }
   ],
   "source": [
    "# Standardize data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(df)\n",
    "print('After Standardization:')\n",
    "np.set_printoptions(precision=3)\n",
    "print(standardized_data[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize Data\n",
    "\n",
    "Normalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called a unit norm in linear algebra).\n",
    "\n",
    "This preprocessing can be useful for sparse datasets (lots of zeros) with attributes of varying scales when using algorithms that weight input values such as neural networks and algorithms that use distance measures such as K-Nearest Neighbors.\n",
    "\n",
    "#### The rows are normalized to length 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Normalizing :\n",
      "[[ 0.604  0.076  0.516  0.447  0.403]\n",
      " [ 0.602  0.52   0.068  0.533  0.28 ]\n",
      " [ 0.037  0.475  0.304  0.453  0.69 ]\n",
      " [ 0.532  0.049  0.106  0.769  0.335]\n",
      " [ 0.421  0.706  0.076  0.252  0.505]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "scaler = Normalizer().fit(df)\n",
    "normalizedX = scaler.transform(df)\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print('After Normalizing :')\n",
    "print(normalizedX[0:5,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarize Data (Make Binary)\n",
    "\n",
    "You can transform your data using a binary threshold. All values above the threshold are marked 1 and all equal to or below are marked as 0.\n",
    "\n",
    "This is called binarizing your data or threshold your data. It can be useful when you have probabilities that you want to make crisp values. It is also useful when feature engineering and you want to add new features that indicate something meaningful.\n",
    "\n",
    "#### You can see that all values equal or less than the threshold are marked 0 and all of those above 0 are marked 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 1 1]\n",
      " [1 1 0 1 0]\n",
      " [0 1 0 1 1]\n",
      " [1 0 0 1 0]\n",
      " [0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# binarization\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "binarizer = Binarizer(threshold=50).fit(df)\n",
    "binaryX = binarizer.transform(df)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle: https://machinelearningmastery.com/prepare-data-machine-learning-python-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdeeplearning",
   "language": "python",
   "name": "tfdeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
